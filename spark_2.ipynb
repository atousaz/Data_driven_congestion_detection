{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congestion detection\n",
    "Atousa Zarindast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd=pd.read_csv('./Cong_interstate/1485903433.csv',header=None)\n",
    "df_pd.columns=['Segment',\n",
    "                                      'C_value',\n",
    "                                      'segco',\n",
    "                                      'Score',\n",
    "                                      'Speed',\n",
    "                                      'Average',\n",
    "                                          'Ref',\n",
    "                                         'Travel',\n",
    "                                          'time',\n",
    "              'ct']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd=df_pd[['Segment','C_value','Speed','Ref','time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Segment</th>\n",
       "      <th>C_value</th>\n",
       "      <th>Speed</th>\n",
       "      <th>Ref</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1485903433</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>2018-07-01 00:09:10 CDT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1485903433</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>2018-07-01 00:10:04 CDT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1485903433</td>\n",
       "      <td>59.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>2018-07-01 00:11:09 CDT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1485903433</td>\n",
       "      <td>59.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>2018-07-01 00:12:00 CDT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1485903433</td>\n",
       "      <td>59.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>2018-07-01 00:12:00 CDT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Segment  C_value  Speed   Ref                     time\n",
       "0  1485903433      NaN   53.0  53.0  2018-07-01 00:09:10 CDT\n",
       "1  1485903433      NaN   53.0  53.0  2018-07-01 00:10:04 CDT\n",
       "2  1485903433     59.0   53.0  53.0  2018-07-01 00:11:09 CDT\n",
       "3  1485903433     59.0   53.0  53.0  2018-07-01 00:12:00 CDT\n",
       "4  1485903433     59.0   53.0  53.0  2018-07-01 00:12:00 CDT"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(df_pd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-----+----+--------------------+\n",
      "|   Segment|C_value|Speed| Ref|                time|\n",
      "+----------+-------+-----+----+--------------------+\n",
      "|1485903433|    NaN| 53.0|53.0|2018-07-01 00:09:...|\n",
      "|1485903433|    NaN| 53.0|53.0|2018-07-01 00:10:...|\n",
      "|1485903433|   59.0| 53.0|53.0|2018-07-01 00:11:...|\n",
      "|1485903433|   59.0| 53.0|53.0|2018-07-01 00:12:...|\n",
      "|1485903433|   59.0| 53.0|53.0|2018-07-01 00:12:...|\n",
      "+----------+-------+-----+----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "conf = SparkConf()\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .config(conf=conf) \\\n",
    "        .appName('Dataframe with Indexes') \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "# the window is necessary here because row_number is a windowing function\n",
    "# that means you can have row_number run over some amount of your data\n",
    "# we'll be currently running it over the sorted by column1 data, row per row - our window will be of size 2 (rows),\n",
    "# the whole dataframe that is.\n",
    "window = Window.orderBy(F.col('time'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('row_number', F.row_number().over(window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-----+----+--------------------+----------+\n",
      "|   Segment|C_value|Speed| Ref|                time|row_number|\n",
      "+----------+-------+-----+----+--------------------+----------+\n",
      "|1485903433|    NaN| 53.0|53.0|2018-01-10 00:02:...|         1|\n",
      "|1485903433|    NaN| 53.0|53.0|2018-01-10 00:08:...|         2|\n",
      "|1485903433|  100.0| 50.0|53.0|2018-01-10 00:09:...|         3|\n",
      "|1485903433|  100.0| 50.0|53.0|2018-01-10 00:10:...|         4|\n",
      "|1485903433|  100.0| 50.0|53.0|2018-01-10 00:14:...|         5|\n",
      "+----------+-------+-----+----+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "String date to timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf, to_timestamp\n",
    "date_format = \"yyyy-MM-dd HH:mm:ss\"\n",
    "df_date=df.withColumn(\"timestamp\", to_timestamp(\"time\", date_format))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criteria of congestion with col D which is speed below 54"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f\n",
    "df_date=df_date.withColumn('D', f.when(f.col('Speed') < 54, 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window as Window\n",
    "w = Window.partitionBy(\"Segment\").orderBy(\"timestamp\")\n",
    "from pyspark.sql import functions as func\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+\n",
      "|          timestamp|  D|\n",
      "+-------------------+---+\n",
      "|2018-07-01 00:09:10|  1|\n",
      "|2018-07-01 00:10:04|  1|\n",
      "|2018-07-01 00:11:09|  1|\n",
      "|2018-07-01 00:12:00|  1|\n",
      "|2018-07-01 00:12:00|  1|\n",
      "|2018-07-01 00:12:07|  1|\n",
      "|2018-07-01 00:13:23|  0|\n",
      "|2018-07-01 00:14:00|  0|\n",
      "|2018-07-01 00:14:06|  1|\n",
      "|2018-07-01 00:15:03|  1|\n",
      "+-------------------+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_date.select('timestamp','D').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make second wise timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-----+----+--------------------+---+-------------------+--------------+\n",
      "|   Segment|C_value|Speed| Ref|                time|  D|          timestamp|timestamp_long|\n",
      "+----------+-------+-----+----+--------------------+---+-------------------+--------------+\n",
      "|1485903433|    NaN| 53.0|53.0|2018-07-01 00:09:...|  1|2018-07-01 00:09:10|    1530403750|\n",
      "|1485903433|    NaN| 53.0|53.0|2018-07-01 00:10:...|  1|2018-07-01 00:10:04|    1530403804|\n",
      "|1485903433|   59.0| 53.0|53.0|2018-07-01 00:11:...|  1|2018-07-01 00:11:09|    1530403869|\n",
      "|1485903433|   59.0| 53.0|53.0|2018-07-01 00:12:...|  1|2018-07-01 00:12:00|    1530403920|\n",
      "|1485903433|   59.0| 53.0|53.0|2018-07-01 00:12:...|  1|2018-07-01 00:12:00|    1530403920|\n",
      "+----------+-------+-----+----+--------------------+---+-------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_try = df_date.withColumn('timestamp_long',df_date.timestamp.astype('Timestamp').cast(\"long\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+\n",
      "|          timestamp|  D|\n",
      "+-------------------+---+\n",
      "|2018-07-01 00:09:10|  1|\n",
      "|2018-07-01 00:10:04|  1|\n",
      "|2018-07-01 00:11:09|  1|\n",
      "|2018-07-01 00:12:00|  1|\n",
      "|2018-07-01 00:12:00|  1|\n",
      "|2018-07-01 00:12:07|  1|\n",
      "|2018-07-01 00:13:23|  0|\n",
      "|2018-07-01 00:14:00|  0|\n",
      "|2018-07-01 00:14:06|  1|\n",
      "|2018-07-01 00:15:03|  1|\n",
      "|2018-07-01 00:16:00|  1|\n",
      "|2018-07-01 00:16:01|  1|\n",
      "|2018-07-01 00:17:05|  1|\n",
      "|2018-07-01 00:18:00|  1|\n",
      "|2018-07-01 00:18:00|  1|\n",
      "|2018-07-01 00:18:01|  1|\n",
      "|2018-07-01 00:19:05|  1|\n",
      "|2018-07-01 00:20:00|  1|\n",
      "|2018-07-01 00:20:00|  1|\n",
      "|2018-07-01 00:20:01|  1|\n",
      "+-------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_date.select('timestamp','D').show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Between 5 min interval count the number of congestion criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+\n",
      "|          timestamp|occurrences_in_5_min|\n",
      "+-------------------+--------------------+\n",
      "|2018-01-10 00:02:08|                   1|\n",
      "|2018-01-10 00:08:08|                   1|\n",
      "|2018-01-10 00:09:07|                   2|\n",
      "|2018-01-10 00:10:06|                   3|\n",
      "|2018-01-10 00:14:07|                   3|\n",
      "|2018-01-10 00:15:04|                   3|\n",
      "|2018-01-10 00:16:04|                   3|\n",
      "|2018-01-10 00:17:04|                   4|\n",
      "|2018-01-10 00:18:04|                   5|\n",
      "|2018-01-10 00:19:06|                   6|\n",
      "+-------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w2 = Window.partitionBy('Segment').orderBy('timestamp_long').rangeBetween(-60*5,0)\n",
    "\n",
    "df_try = df_try.withColumn('occurrences_in_5_min',F.count('D').over(w2))\n",
    "df_try.select('timestamp','occurrences_in_5_min').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if count of congestion criteria whitin 5 min is above 5 give me an indicator (this defines whether if all the minutes in that interval are congested continuesly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------------+\n",
      "|          timestamp|already_occured|\n",
      "+-------------------+---------------+\n",
      "|2018-01-10 00:02:08|          false|\n",
      "|2018-01-10 00:08:08|          false|\n",
      "|2018-01-10 00:09:07|          false|\n",
      "|2018-01-10 00:10:06|          false|\n",
      "|2018-01-10 00:14:07|          false|\n",
      "|2018-01-10 00:15:04|          false|\n",
      "|2018-01-10 00:16:04|          false|\n",
      "|2018-01-10 00:17:04|          false|\n",
      "|2018-01-10 00:18:04|          false|\n",
      "|2018-01-10 00:19:06|           null|\n",
      "+-------------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "add_bool = udf(lambda col: 1 if col>5 else False, BooleanType())\n",
    "df_try = df_try.withColumn('already_occured',add_bool('occurrences_in_5_min'))\n",
    "df_try.select('timestamp','already_occured').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn the true/false to boolian as indicator col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+\n",
      "|          timestamp|indicator|\n",
      "+-------------------+---------+\n",
      "|2018-01-10 00:02:08|        0|\n",
      "|2018-01-10 00:08:08|        0|\n",
      "|2018-01-10 00:09:07|        0|\n",
      "|2018-01-10 00:10:06|        0|\n",
      "|2018-01-10 00:14:07|        0|\n",
      "|2018-01-10 00:15:04|        0|\n",
      "|2018-01-10 00:16:04|        0|\n",
      "|2018-01-10 00:17:04|        0|\n",
      "|2018-01-10 00:18:04|        0|\n",
      "|2018-01-10 00:19:06|        1|\n",
      "+-------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "add_bool_1 = udf(lambda col: 1 if col>5 else 0, IntegerType())\n",
    "df_try = df_try.withColumn('indicator',add_bool_1('occurrences_in_5_min'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduce col p which is the time that congestion started (start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f\n",
    "w4 = Window.partitionBy('Segment').orderBy('timestamp')\n",
    "from pyspark.sql.functions import col, expr, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_try_2=df_try.withColumn(\"p\",when((df_try.indicator==1),f.lag(df_try.timestamp,5).over(w4)).otherwise(\"null\"))\n",
    "                                \n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+-------------------+\n",
      "|          timestamp|indicator|                  p|\n",
      "+-------------------+---------+-------------------+\n",
      "|2018-01-10 00:02:08|        0|               null|\n",
      "|2018-01-10 00:08:08|        0|               null|\n",
      "|2018-01-10 00:09:07|        0|               null|\n",
      "|2018-01-10 00:10:06|        0|               null|\n",
      "|2018-01-10 00:14:07|        0|               null|\n",
      "|2018-01-10 00:15:04|        0|               null|\n",
      "|2018-01-10 00:16:04|        0|               null|\n",
      "|2018-01-10 00:17:04|        0|               null|\n",
      "|2018-01-10 00:18:04|        0|               null|\n",
      "|2018-01-10 00:19:06|        1|2018-01-10 00:14:07|\n",
      "|2018-01-10 00:20:06|        0|               null|\n",
      "|2018-01-10 00:22:08|        0|               null|\n",
      "|2018-01-10 00:23:13|        0|               null|\n",
      "|2018-01-10 00:24:01|        0|               null|\n",
      "|2018-01-10 00:25:00|        1|2018-01-10 00:19:06|\n",
      "|2018-01-10 00:25:00|        1|2018-01-10 00:20:06|\n",
      "|2018-01-10 00:25:00|        1|2018-01-10 00:22:08|\n",
      "|2018-01-10 00:25:01|        1|2018-01-10 00:23:13|\n",
      "|2018-01-10 00:29:05|        0|               null|\n",
      "|2018-01-10 00:33:05|        0|               null|\n",
      "+-------------------+---------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_try_2.select('timestamp','indicator','p').show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puting start time and end time altogether please note that congestion criteria here is speed below 54 for consequative 5 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|                  p|          timestamp|\n",
      "+-------------------+-------------------+\n",
      "|2018-01-10 00:14:07|2018-01-10 00:19:06|\n",
      "|2018-01-10 00:19:06|2018-01-10 00:25:00|\n",
      "|2018-01-10 00:20:06|2018-01-10 00:25:00|\n",
      "|2018-01-10 00:22:08|2018-01-10 00:25:00|\n",
      "|2018-01-10 00:23:13|2018-01-10 00:25:01|\n",
      "|2018-01-10 00:33:05|2018-01-10 00:37:07|\n",
      "|2018-01-10 00:34:00|2018-01-10 00:38:05|\n",
      "|2018-01-10 00:34:05|2018-01-10 00:39:05|\n",
      "|2018-01-10 00:35:04|2018-01-10 00:40:04|\n",
      "|2018-01-10 00:36:11|2018-01-10 00:41:06|\n",
      "+-------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_try_2.filter(df_try_2.indicator==1).select('p','timestamp').show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RC detection based on probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separating date and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format\n",
    "data=df_try_2.withColumn(\"date\",date_format('timestamp', 'yyyy-MM-dd')).withColumn(\"time_separated\",date_format('timestamp', 'HH:mm:ss'))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+----------+-------------------+--------------+\n",
      "|indicator|                time|      date|          timestamp|time_separated|\n",
      "+---------+--------------------+----------+-------------------+--------------+\n",
      "|        0|2018-01-10 00:02:...|2018-01-10|2018-01-10 00:02:08|      00:02:08|\n",
      "|        0|2018-01-10 00:08:...|2018-01-10|2018-01-10 00:08:08|      00:08:08|\n",
      "|        0|2018-01-10 00:09:...|2018-01-10|2018-01-10 00:09:07|      00:09:07|\n",
      "|        0|2018-01-10 00:10:...|2018-01-10|2018-01-10 00:10:06|      00:10:06|\n",
      "+---------+--------------------+----------+-------------------+--------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select('indicator','time','date','timestamp','time_separated').show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------+\n",
      "|time_separated|occourance_p|\n",
      "+--------------+------------+\n",
      "|      03:50:07|          20|\n",
      "|      05:00:08|          14|\n",
      "|      05:11:20|           0|\n",
      "|      09:19:23|           1|\n",
      "|      15:50:05|          23|\n",
      "|      19:24:06|          19|\n",
      "|      00:41:00|          21|\n",
      "|      10:12:05|          18|\n",
      "|      16:49:10|           7|\n",
      "|      20:08:09|          15|\n",
      "+--------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.groupBy(\"time_separated\").agg(f.sum('indicator').alias(\"occourance_p\")).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+---------+\n",
      "|time_separated|      date|indicator|\n",
      "+--------------+----------+---------+\n",
      "|      00:02:08|2018-01-10|        0|\n",
      "|      00:08:08|2018-01-10|        0|\n",
      "|      00:09:07|2018-01-10|        0|\n",
      "|      00:10:06|2018-01-10|        0|\n",
      "|      00:14:07|2018-01-10|        0|\n",
      "+--------------+----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select('time_separated','date','indicator').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_partition= Window.partitionBy('time_separated')\n",
    "data1=data.withColumn('occurance', f.sum('indicator').over(time_partition))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|      date|occurance|\n",
      "+----------+---------+\n",
      "|2018-11-14|        0|\n",
      "|2018-05-16|        1|\n",
      "|2018-04-27|        6|\n",
      "|2018-04-30|        6|\n",
      "+----------+---------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1.select('date','occurance').show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_partition= Window.partitionBy('time_separated')\n",
    "data1=data1.withColumn('occurance_all', f.count('date').over(time_partition))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(Segment,LongType,true),StructField(C_value,DoubleType,true),StructField(Speed,DoubleType,true),StructField(Ref,DoubleType,true),StructField(time,StringType,true),StructField(D,IntegerType,false),StructField(timestamp,TimestampType,true),StructField(timestamp_long,LongType,true),StructField(occurrences_in_5_min,LongType,false),StructField(already_occured,BooleanType,true),StructField(indicator,IntegerType,true),StructField(p,StringType,true),StructField(date,StringType,true),StructField(time_separated,StringType,true),StructField(occurance,LongType,true),StructField(occurance_p,LongType,false),StructField(occurance_all,LongType,false)))"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+------------------+----------+--------------+\n",
      "|occurance|occurance_all|       occurance_p|      date|time_separated|\n",
      "+---------+-------------+------------------+----------+--------------+\n",
      "|        0|            1|               0.0|2018-11-14|      00:01:48|\n",
      "|        1|            1|               1.0|2018-05-16|      00:07:15|\n",
      "|        6|           11|0.5454545454545454|2018-04-27|      00:29:10|\n",
      "|        6|           11|0.5454545454545454|2018-04-30|      00:29:10|\n",
      "|        6|           11|0.5454545454545454|2018-05-01|      00:29:10|\n",
      "+---------+-------------+------------------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1.withColumn('occurance_p',f.col('occurance')/f.col('occurance_all')).select('occurance','occurance_all','occurance_p','date','time_separated').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2=data1.withColumn('occurance_p',f.col('occurance')/f.col('occurance_all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.repartition(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"spark.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# finding RC\n",
    "for timetamps with congested probability higher than 0.2 we identify continues congestion for concurrent 5 min and report start and end time\n",
    "this would be combination of previous two step, as such that instead of just having congestion criteria we care for congested for continues 5 min and probability higher than 0.2 for that time period as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability in 5 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2 = Window.partitionBy('Segment').orderBy('timestamp_long').rangeBetween(-60*5,0)\n",
    "\n",
    "data3 = data2.withColumn('prob_in_5_min',F.sum('occurance_p').over(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "data4 = data3.withColumn('RC_indicator', f.when((data3.occurrences_in_5_min >5)& (data3.prob_in_5_min>1),1).otherwise(0) )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+--------------+------------+\n",
      "|occurrences_in_5_min|     prob_in_5_min|time_separated|RC_indicator|\n",
      "+--------------------+------------------+--------------+------------+\n",
      "|                   1|0.3333333333333333|      00:02:08|           0|\n",
      "|                   1|              0.64|      00:08:08|           0|\n",
      "|                   2| 1.281025641025641|      00:09:07|           0|\n",
      "|                   3|1.6976923076923078|      00:10:06|           0|\n",
      "|                   3|1.7984330484330484|      00:14:07|           0|\n",
      "|                   3|2.0487117552334944|      00:15:04|           0|\n",
      "|                   3| 2.401275857797597|      00:16:04|           0|\n",
      "|                   4|3.0499245064462457|      00:17:04|           0|\n",
      "|                   5|3.9165911731129124|      00:18:04|           0|\n",
      "|                   6|4.5984093549310945|      00:19:06|           1|\n",
      "+--------------------+------------------+--------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data4.select('occurrences_in_5_min','prob_in_5_min','time_separated','RC_indicator').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "showing the time that RC defition happend with start and end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|      RC_start_time|          timestamp|\n",
      "+-------------------+-------------------+\n",
      "|2018-01-10 00:14:07|2018-01-10 00:19:06|\n",
      "|2018-01-10 00:19:06|2018-01-10 00:25:00|\n",
      "|2018-01-10 00:20:06|2018-01-10 00:25:00|\n",
      "|2018-01-10 00:22:08|2018-01-10 00:25:00|\n",
      "|2018-01-10 00:23:13|2018-01-10 00:25:01|\n",
      "|2018-01-10 00:33:05|2018-01-10 00:37:07|\n",
      "|2018-01-10 00:34:00|2018-01-10 00:38:05|\n",
      "|2018-01-10 00:34:05|2018-01-10 00:39:05|\n",
      "|2018-01-10 00:35:04|2018-01-10 00:40:04|\n",
      "|2018-01-10 00:36:11|2018-01-10 00:41:06|\n",
      "+-------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w4 = Window.partitionBy('Segment').orderBy('timestamp')\n",
    "data4=data4.withColumn(\"RC_start_time\",when((data4.RC_indicator==1),f.lag(data4.timestamp,5).over(w4)).otherwise(\"null\"))\n",
    "\n",
    "data4.filter(data4.RC_indicator==1).select('RC_start_time','timestamp').show(10)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|avg_speed|\n",
      "+---------+\n",
      "|      NaN|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data4.when(data4.RC_indicator==1,agg(f.mean('Speed')).otherwise(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
